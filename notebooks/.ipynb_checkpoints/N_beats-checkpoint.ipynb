{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a545ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a4a4478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_params = {'data': {'len_input': 100, 'horizon_forecast': 20, 'size_train': 0.8, 'size_valid': 0.2},\n",
    "               'model': {'n_comp_trend': 3, 'n_neur_hidden': 100, 'frac_dropout': 0.2},\n",
    "               'training': {'batch_size': 128, 'n_epochs': 500, 'patience': 50, 'min_delta_loss_perc': 0.01}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c1052585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, dict_params: dict, component: str, num_features: int) -> None:\n",
    "        '''\n",
    "        Block of the N-BEATS architecture.\n",
    "        \n",
    "        Args:\n",
    "            dict_params: Dictionary containing the configuration settings.\n",
    "            component: String representing the component to be modelled; it can be either 'trend' or 'seasonality'.\n",
    "            num_features: Number of features of the input series.\n",
    "            \n",
    "        Returns: None\n",
    "        '''\n",
    "        super().__init__()\n",
    "        #\n",
    "        len_input = dict_params['data']['len_input']\n",
    "        horizon_forecast = dict_params['data']['horizon_forecast']\n",
    "        n_comp_trend = dict_params['model']['n_comp_trend']\n",
    "        n_neur_hidden = dict_params['model']['n_neur_hidden']\n",
    "        frac_dropout = dict_params['model']['frac_dropout']\n",
    "        self.component = component\n",
    "        self.horizon_forecast = horizon_forecast\n",
    "        self.n_comp_trend = n_comp_trend\n",
    "        # hidden layers of the FC stack\n",
    "        self.dense_hid_1 = torch.nn.Linear(in_features = len_input*num_features, out_features = n_neur_hidden)\n",
    "        self.dense_hid_2 = torch.nn.Linear(in_features = n_neur_hidden, out_features = n_neur_hidden)\n",
    "        self.dense_hid_3 = torch.nn.Linear(in_features = n_neur_hidden, out_features = n_neur_hidden)\n",
    "        self.dense_hid_4 = torch.nn.Linear(in_features = n_neur_hidden, out_features = n_neur_hidden)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p = frac_dropout)\n",
    "        self.batch_norm_1 = torch.nn.BatchNorm1d(n_neur_hidden)\n",
    "        self.batch_norm_2 = torch.nn.BatchNorm1d(n_neur_hidden)\n",
    "        self.batch_norm_3 = torch.nn.BatchNorm1d(n_neur_hidden)\n",
    "        self.batch_norm_4 = torch.nn.BatchNorm1d(n_neur_hidden)\n",
    "        # dense layer for producing the theta's\n",
    "        self.dense_theta_b = torch.nn.Linear(in_features = n_neur_hidden, out_features = n_neur_hidden, bias = False)\n",
    "        if component == 'trend':\n",
    "            self.dense_theta_f = torch.nn.Linear(in_features = n_neur_hidden, out_features = self.n_comp_trend + 1, bias = False)\n",
    "        if component == 'seasonality':\n",
    "            self.dense_theta_f = torch.nn.Linear(in_features = n_neur_hidden, out_features = 2*int(np.floor(horizon_forecast/2-1)) + 1, bias = False)\n",
    "        # trend and seasonality matrices\n",
    "        self.mat_T = np.hstack([(np.arange(horizon_forecast).reshape(-1, 1)/horizon_forecast)**p for p in range(n_comp_trend+1)]).astype(np.float32)\n",
    "        self.mat_S = np.hstack([np.arange(horizon_forecast).reshape(-1, 1)**0] +\n",
    "                               [np.cos(2*np.pi*i*np.arange(horizon_forecast).reshape(-1, 1)/horizon_forecast) for i in\n",
    "                                range(1, int(np.floor(horizon_forecast/2-1)) + 1)] +\n",
    "                               [np.sin(2*np.pi*i*np.arange(horizon_forecast).reshape(-1, 1)/horizon_forecast) for i in\n",
    "                                range(1, int(np.floor(horizon_forecast/2-1)) + 1)]).astype(np.float32)\n",
    "        self.mat_T, self.mat_S = torch.tensor(self.mat_T), torch.tensor(self.mat_S)\n",
    "        # final layer for backcasting\n",
    "        self.dense_x_hat = torch.nn.Linear(in_features = n_neur_hidden, out_features = len_input)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        #\n",
    "        y = self.dense_hid_1(x)\n",
    "        y = self.batch_norm_1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.dropout(y)\n",
    "        for i in range(3):\n",
    "            hidden_layer = [self.dense_hid_2, self.dense_hid_3, self.dense_hid_4][i]\n",
    "            batch_norm = [self.batch_norm_2, self.batch_norm_3, self.batch_norm_4][i]\n",
    "            y = hidden_layer(y)\n",
    "            y = batch_norm(y)\n",
    "            y = self.relu(y)\n",
    "            y = self.dropout(y)\n",
    "        # compute theta's\n",
    "        theta_b = self.dense_theta_b(y)\n",
    "        theta_f = self.dense_theta_f(y)\n",
    "        # compute backcast\n",
    "        x_hat = self.dense_x_hat(theta_b)\n",
    "        x_hat = x_hat.reshape(*x_hat.shape, 1)\n",
    "        # compute time series components\n",
    "        if self.component == 'trend':\n",
    "            y_hat = torch.matmul(self.mat_T.repeat(theta_f.shape[0], 1, 1), theta_f.reshape(*theta_f.shape, 1))\n",
    "        if self.component == 'seasonality':\n",
    "            y_hat = torch.matmul(self.mat_S.repeat(theta_f.shape[0], 1, 1), theta_f.reshape(*theta_f.shape, 1))\n",
    "        #\n",
    "        return x_hat, y_hat\n",
    "\n",
    "class Stack(torch.nn.Module):\n",
    "    def __init__(self, dict_params: dict, component: str, num_features: int) -> None:\n",
    "        '''\n",
    "        Stack of the N-BEATS architecture.\n",
    "        \n",
    "        Args:\n",
    "            dict_params: Dictionary containing the configuration settings.\n",
    "            component: String representing the component to be modelled; it can be either 'trend' or 'seasonality'.\n",
    "            num_features: Number of features of the input series.\n",
    "            \n",
    "        Returns: None\n",
    "        '''\n",
    "        super().__init__()\n",
    "        #\n",
    "        self.block_1 = Block(dict_params = dict_params, component = component, num_features = num_features)\n",
    "        self.block_2 = Block(dict_params = dict_params, component = component, num_features = num_features)\n",
    "        self.block_3 = Block(dict_params = dict_params, component = component, num_features = num_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # first block\n",
    "        x_hat, y = self.block_1(x)\n",
    "        x = x - x_hat\n",
    "        y_hat = y\n",
    "        # second block\n",
    "        x_hat, y = self.block_2(x)\n",
    "        x = x - x_hat\n",
    "        y_hat += y\n",
    "        # third block\n",
    "        x_hat, y = self.block_3(x)\n",
    "        x = x - x_hat\n",
    "        y_hat += y\n",
    "        #\n",
    "        x_hat = x\n",
    "        return x_hat, y_hat\n",
    "\n",
    "class NBeats(torch.nn.Module):\n",
    "    def __init__(self, dict_params: dict, num_features: int) -> None:\n",
    "        '''\n",
    "        N-BEATS architecture.\n",
    "        \n",
    "        Args:\n",
    "            dict_params: Dictionary containing the configuration settings.\n",
    "            num_features: Number of features of the input series.\n",
    "            \n",
    "        Returns: None\n",
    "        '''\n",
    "        super().__init__()\n",
    "        #\n",
    "        self.stack_trend = Stack(dict_params = dict_params, component = 'trend', num_features = num_features)\n",
    "        self.stack_seas = Stack(dict_params = dict_params, component = 'seasonality', num_features = num_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # trend stack\n",
    "        x_hat, y_hat_trend = self.stack_trend(x)\n",
    "        x = x - x_hat\n",
    "        # seasonality stack\n",
    "        _, y_hat_seas = self.stack_seas(x)\n",
    "        #\n",
    "        return y_hat_trend, y_hat_seas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f1204525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splitting(df: pd.DataFrame, dict_params: dict) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    '''\n",
    "    Function to split data in training, validation and test set.\n",
    "\n",
    "    Args:\n",
    "        df: Dataframe containing time series. The column representing the main series should be called `y`.\n",
    "        dict_params: Dictionary containing information about the model architecture.\n",
    "\n",
    "    Returns:\n",
    "        df_train: Dataframe corresponding to training set.\n",
    "        df_valid: Dataframe corresponding to validation set.\n",
    "        df_test: Dataframe corresponding to test set.\n",
    "    '''\n",
    "    df_train = df.iloc[:int(df.shape[0]*dict_params['data']['size_train'])].copy().reset_index(drop = True)\n",
    "    df_test = df.iloc[int(df.shape[0]*dict_params['data']['size_train']):].reset_index(drop = True).copy().reset_index(drop = True)\n",
    "    df_valid = df_train.iloc[int(df_train.shape[0]*(1 - dict_params['data']['size_valid'])):].copy().reset_index(drop = True)\n",
    "    df_train = df_train.iloc[:int(df_train.shape[0]*(1 - dict_params['data']['size_valid']))].copy().reset_index(drop = True)\n",
    "    # rescale data\n",
    "    scaler = StandardScaler().fit(df_train[['y']])\n",
    "    df_train[['y']] = scaler.transform(df_train[['y']])\n",
    "    df_valid[['y']] = scaler.transform(df_valid[['y']])\n",
    "    df_test[['y']] = scaler.transform(df_test[['y']])\n",
    "    #\n",
    "    return df_train, df_valid, df_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6673cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y(df: pd.DataFrame, df_future: pd.DataFrame, dict_params: dict, test_set: bool = False,\n",
    "            horizon_forecast: int = None) -> (torch.tensor, torch.tensor, np.array, np.array):\n",
    "    '''\n",
    "    Function obtain a tensor of regressors and one of target series.\n",
    "\n",
    "    Args:\n",
    "        df: Dataframe containing time series. The column representing the main series should be called `y`.\n",
    "        df_future: Same as `df`, but corresponding to its future (e.g., `df_valid` could be the \"future\" of `df_train`).\n",
    "        dict_params: Dictionary containing information about the model architecture.\n",
    "        test_set: Whether `df` is the dataframe corresponding to test set.\n",
    "        horizon_forecast: Length of the series to be predicted.\n",
    "\n",
    "    Returns:\n",
    "        x: Tensor representing regressors.\n",
    "        y: Tensor representing target time series.\n",
    "        date_x: Array containing the dates corresponding to the elements of `x`.\n",
    "        date_y: Array containing the dates corresponding to the elements of `y`.\n",
    "    '''\n",
    "    dict_params = dict_params['data']\n",
    "    #\n",
    "    len_input = dict_params['len_input']\n",
    "    if (horizon_forecast is None) or (horizon_forecast >= len_input):\n",
    "        horizon_forecast = len_input\n",
    "    #\n",
    "    df_present = df.copy()\n",
    "    if test_set == False:\n",
    "        df_future = pd.concat((df_present, df_future)).copy().shift(-len_input)\n",
    "        df_future = df_future.iloc[:df_present.shape[0]].reset_index(drop = True)\n",
    "    else:\n",
    "        df_future = df_present.copy().shift(-len_input).dropna()\n",
    "        df_present = df_present.iloc[:df_future.shape[0]]\n",
    "    #\n",
    "    x = np.array([df_present.loc[i: i + len_input - 1,\n",
    "                                 [col for col in df_present.columns if col != 'date']].values for i in range(df_present.shape[0] - len_input)])\n",
    "    y = np.array([df_future.loc[i: i + horizon_forecast - 1, ['y']].values for i in range(df_future.shape[0] - horizon_forecast)])\n",
    "    date_x = np.array([df_present.loc[i: i + len_input - 1, 'date'].values for i in range(df_present.shape[0] - len_input)])\n",
    "    date_y = np.array([df_future.loc[i: i + horizon_forecast - 1, 'date'].values for i in range(df_future.shape[0] - horizon_forecast)])\n",
    "    #\n",
    "    y = y[:x.shape[0]]\n",
    "    date_y = date_y[:date_x.shape[0]]\n",
    "    #\n",
    "    x = torch.tensor(x.astype(np.float32))\n",
    "    y = torch.tensor(y.astype(np.float32))\n",
    "    #\n",
    "    date_x = date_x.reshape(x.shape[0], -1, 1)\n",
    "    date_y = date_y.reshape(y.shape[0], -1, 1)\n",
    "    #\n",
    "    return x, y, date_x, date_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f948097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        '''\n",
    "        Class to create a PyTorch dataset.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor representing regressors.\n",
    "            y: Tensor representing target time series.\n",
    "            \n",
    "        Returns: None.\n",
    "        '''\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b81cc4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainNBeats:\n",
    "    def __init__(self, model: torch.nn.Module, dict_params: dict, dataloader_train: torch.utils.data.DataLoader, dataloader_valid: torch.utils.data.DataLoader):\n",
    "        '''\n",
    "        Class to train the N-BEATS model.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch model.\n",
    "            dict_params: Dictionary containing information about the model architecture.\n",
    "            dataloader_train: Dataloader containing training data.\n",
    "            dataloader_valid: Dataloader containing validation data.\n",
    "            \n",
    "        Returns: None.\n",
    "        '''\n",
    "        self.model = model\n",
    "        self.dict_params = dict_params\n",
    "        self.loss_func = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(params = model.parameters())\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = self.optimizer, factor = 0.5)\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_valid = dataloader_valid\n",
    "\n",
    "    def _model_on_batch(self, batch: tuple, training: bool, loss_epoch: float) -> float:\n",
    "        '''\n",
    "        Function to perform training on a single batch of data.\n",
    "        \n",
    "        Args:\n",
    "            batch: Batch of data to use for training/evaluation.\n",
    "            training: Whether to perform training (if not, evaluation is understood).\n",
    "            loss_epoch: Loss of the current epoch.\n",
    "            \n",
    "        Returns:\n",
    "            loss: Value of the loss function.\n",
    "        '''\n",
    "        if training == True:\n",
    "            self.optimizer.zero_grad()\n",
    "        #\n",
    "        x, y_true = batch\n",
    "        x = x.to('cpu')\n",
    "        y_true = y_true.to('cpu')\n",
    "        y_hat_trend, y_hat_seas = self.model(x)\n",
    "        y_hat_trend = y_hat_trend.to('cpu')\n",
    "        y_hat_seas = y_hat_seas.to('cpu')\n",
    "        #\n",
    "        loss = self.loss_func(y_hat_trend + y_hat_seas, y_true)\n",
    "        #\n",
    "        if training == True:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        #\n",
    "        return loss.item()\n",
    "\n",
    "    def _train(self) -> float:\n",
    "        '''\n",
    "        Function to train the N-BEATS model on a single epoch.\n",
    "        \n",
    "        Args: None.\n",
    "            \n",
    "        Returns:\n",
    "            loss: Value of the training loss function per batch.\n",
    "        '''\n",
    "        self.model.train()\n",
    "        loss_epoch = 0\n",
    "        for batch in self.dataloader_train:\n",
    "            loss_epoch += self._model_on_batch(batch = batch, training = True, loss_epoch = loss_epoch)\n",
    "        return loss_epoch/len(self.dataloader_train)\n",
    "\n",
    "    def _eval(self) -> float:\n",
    "        '''\n",
    "        Function to evaluate the N-BEATS model on the validation set on a single epoch.\n",
    "        \n",
    "        Args: None.\n",
    "            \n",
    "        Returns:\n",
    "            loss: Value of the validation loss function per batch.\n",
    "        '''\n",
    "        self.model.eval()\n",
    "        loss_epoch = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in self.dataloader_valid:\n",
    "                loss_epoch += self._model_on_batch(batch = batch, training = False, loss_epoch = loss_epoch)\n",
    "        return loss_epoch/len(self.dataloader_valid)\n",
    "\n",
    "    def train_model(self) -> (torch.nn.Module, list, list):\n",
    "        '''\n",
    "        Function to train the N-BEATS model.\n",
    "        \n",
    "        Args: None.\n",
    "            \n",
    "        Returns:\n",
    "            model: Trained N-BEATS model.\n",
    "            list_loss_train: List of training loss function across the epochs.\n",
    "            list_loss_valid: List of validation loss function across the epochs.\n",
    "        '''\n",
    "        dict_params = self.dict_params\n",
    "        n_epochs = dict_params['training']['n_epochs']\n",
    "        list_loss_train, list_loss_valid = [], []\n",
    "        counter_patience = 0\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            loss_train = self._train()\n",
    "            loss_valid = self._eval()\n",
    "            #\n",
    "            if (len(list_loss_valid) > 0) and (loss_valid >= np.min(list_loss_valid)*(1 - dict_params['training']['min_delta_loss_perc'])):\n",
    "                counter_patience += 1\n",
    "            if (len(list_loss_valid) == 0) or ((len(list_loss_valid) > 0) and (loss_valid < np.min(list_loss_valid))):\n",
    "                counter_patience = 0\n",
    "                torch.save(self.model.state_dict(), '../data/artifacts/weights.p')\n",
    "            if counter_patience >= dict_params['training']['patience']:\n",
    "                print(f'Training stopped at epoch {epoch}. Restoring weights from epoch {np.argmin(list_loss_valid) + 1}.')\n",
    "                self.model.load_state_dict(torch.load('../data/artifacts/weights.p'))\n",
    "                break\n",
    "            #\n",
    "            print(f'Epoch {epoch}: training loss = {loss_train:.4f}, validation loss = {loss_valid:.4f}, patience counter = {counter_patience}.')\n",
    "            self.scheduler.step(loss_valid)\n",
    "            #\n",
    "            list_loss_train.append(loss_train)\n",
    "            list_loss_valid.append(loss_valid)\n",
    "        if epoch == n_epochs:\n",
    "            self.model.load_state_dict(torch.load('../data/artifacts/weights.p'))\n",
    "        return self.model, list_loss_train, list_loss_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "762195dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_true_y_hat(model: torch.nn.Module, x: torch.tensor, y: torch.tensor, date_y: np.array,\n",
    "                     scaler: sklearn.preprocessing.StandardScaler) -> (np.array, np.array, np.array):\n",
    "    '''\n",
    "    Function to get the real time series and its prediction.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained N-BEATS model.\n",
    "        x: Tensor representing regressors.\n",
    "        y: Tensor representing target time series.\n",
    "        date_y: Array containing the dates corresponding to the elements of `y`.\n",
    "        scaler: Scaled used to rescale data.\n",
    "        \n",
    "    Returns:\n",
    "        y_true: Array containing the true values.\n",
    "        y_hat_trend: Array containing the predicted trend.\n",
    "        y_hat_seas: Array containing the predicted seasonality.\n",
    "    '''\n",
    "    list_date = []\n",
    "    y_true = []\n",
    "    y_hat_trend, y_hat_seas = [], []\n",
    "    pred_trend, pred_seas = model(x)\n",
    "    for i in range(np.unique(date_y).shape[0]):\n",
    "        date = np.unique(date_y)[i]\n",
    "        list_date.append(date)\n",
    "        idx = np.where(date_y == date)\n",
    "        y_true.append(y.numpy()[idx].mean())\n",
    "        y_hat_trend.append(pred_trend.detach().numpy()[idx].mean())\n",
    "        y_hat_seas.append(pred_seas.detach().numpy()[idx].mean())\n",
    "    y_true = np.array(y_true)\n",
    "    y_hat_trend = np.array(y_hat_trend)\n",
    "    y_hat_seas = np.array(y_hat_seas)\n",
    "    # scale back\n",
    "    y_true = scaler.inverse_transform(y_true.reshape(-1, 1)).ravel()\n",
    "    y_hat_trend = scaler.inverse_transform(y_hat_trend.reshape(-1, 1)).ravel()\n",
    "    y_hat_seas = scaler.inverse_transform(y_hat_seas.reshape(-1, 1)).ravel()\n",
    "    #\n",
    "    return y_true, y_hat_trend, y_hat_seas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4a19cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mape(y_true: np.array, y_hat_trend: np.array, y_hat_seas: np.array, scaler: sklearn.preprocessing.StandardScaler) -> float:\n",
    "    '''\n",
    "    Function to compute the MAPE.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Array containing the true values.\n",
    "        y_hat_trend: Array containing the predicted trend.\n",
    "        y_hat_seas: Array containing the predicted seasonality.\n",
    "        scaler: Scaled used to rescale data.\n",
    "        \n",
    "    Returns:\n",
    "        mape: MAPE computed from `y_true` and `y_hat`.\n",
    "    '''\n",
    "    y_hat = y_hat_trend + y_hat_seas - scaler.mean_\n",
    "    mape = np.mean(abs(y_true[y_true > 0] - y_hat[y_true > 0])/y_true[y_true > 0])\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5191796c-b7f6-461b-8891-a3fa77bc9b2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 6.1141, validation loss = 1.0291, patience counter = 0.\n",
      "Epoch 2: training loss = 4.5831, validation loss = 1.0881, patience counter = 1.\n",
      "Epoch 3: training loss = 3.8431, validation loss = 1.0520, patience counter = 2.\n",
      "Epoch 4: training loss = 3.1033, validation loss = 0.9169, patience counter = 0.\n",
      "Epoch 5: training loss = 2.7442, validation loss = 0.7774, patience counter = 0.\n",
      "Epoch 6: training loss = 2.3734, validation loss = 0.7229, patience counter = 0.\n",
      "Epoch 7: training loss = 2.0677, validation loss = 0.6435, patience counter = 0.\n",
      "Epoch 8: training loss = 1.8236, validation loss = 0.6155, patience counter = 0.\n",
      "Epoch 9: training loss = 1.6775, validation loss = 0.5503, patience counter = 0.\n",
      "Epoch 10: training loss = 1.4903, validation loss = 0.5193, patience counter = 0.\n",
      "Epoch 11: training loss = 1.4015, validation loss = 0.5242, patience counter = 1.\n",
      "Epoch 12: training loss = 1.2644, validation loss = 0.4902, patience counter = 0.\n",
      "Epoch 13: training loss = 1.1831, validation loss = 0.4548, patience counter = 0.\n",
      "Epoch 14: training loss = 1.0453, validation loss = 0.4241, patience counter = 0.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw/train.csv', parse_dates = ['date'], index_col = 'id')\n",
    "df = df.groupby(['date', 'family']).agg({'sales': 'sum'}).reset_index()\n",
    "# add all dates\n",
    "df_temp = []\n",
    "for family in df['family'].unique():\n",
    "    df_temp.append(pd.DataFrame({'date': pd.date_range(df['date'].min(), df['date'].max())}).merge(df[df['family'] == family],\n",
    "                                                                                                on = 'date', how = 'left'))\n",
    "    df_temp[-1]['family'] = family\n",
    "    df_temp[-1]['sales'] = df_temp[-1]['sales'].ffill()\n",
    "df = pd.concat(df_temp).reset_index(drop = True)\n",
    "del df_temp\n",
    "df = df.rename(columns = {'sales': 'y'})\n",
    "# perform train-test splitting\n",
    "df_train, df_valid, df_test, scaler = train_test_splitting(df = df[df['family'] == 'AUTOMOTIVE'].reset_index(drop = True).drop('family', axis = 1),\n",
    "                                                        dict_params = dict_params)\n",
    "#\n",
    "horizon_forecast = dict_params['data']['horizon_forecast']\n",
    "# training set data\n",
    "x_train, y_train, date_x_train, date_y_train = get_x_y(df = df_train, df_future = df_valid, dict_params = dict_params,\n",
    "                                                    test_set = False, horizon_forecast = horizon_forecast)\n",
    "# validation set data\n",
    "x_valid, y_valid, date_x_valid, date_y_valid = get_x_y(df = df_valid, df_future = df_test, dict_params = dict_params,\n",
    "                                                    test_set = False, horizon_forecast = horizon_forecast)\n",
    "# test set data\n",
    "x_test, y_test, date_x_test, date_y_test = get_x_y(df = df_test, df_future = None, dict_params = dict_params, test_set = True,\n",
    "                                                horizon_forecast = horizon_forecast)\n",
    "# create datasets and dataloader\n",
    "dataset_train = CreateDataset(x = x_train, y = y_train)\n",
    "dataset_valid = CreateDataset(x = x_valid, y = y_valid)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size = dict_params['training']['batch_size'], shuffle = True)\n",
    "dataloader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size = dict_params['training']['batch_size'], shuffle = False)\n",
    "# train model\n",
    "model = NBeats(dict_params = dict_params, num_features = x_train.shape[2])\n",
    "model, list_loss_train, list_loss_valid = TrainNBeats(model = model, dict_params = dict_params, dataloader_train = dataloader_train, dataloader_valid = dataloader_train).train_model()\n",
    "# evaluate results\n",
    "model = NBeats(dict_params = dict_params, num_features = x_train.shape[2])\n",
    "model.load_state_dict(torch.load('../data/artifacts/weights.p'))\n",
    "model.eval()\n",
    "# get time series and the corresponding predictions\n",
    "y_true_train, y_hat_trend_train, y_hat_seas_train = get_y_true_y_hat(model = model, x = x_train, y = y_train, date_y = date_y_train, scaler = scaler)\n",
    "y_true_valid, y_hat_trend_valid, y_hat_seas_valid = get_y_true_y_hat(model = model, x = x_valid, y = y_valid, date_y = date_y_valid, scaler = scaler)\n",
    "y_true_test, y_hat_trend_test, y_hat_seas_test = get_y_true_y_hat(model = model, x = x_test, y = y_test, date_y = date_y_test, scaler = scaler)\n",
    "# compute mape on training, validation and test set\n",
    "mape_train = compute_mape(y_true = y_true_train, y_hat_trend = y_hat_trend_train, y_hat_seas = y_hat_seas_train, scaler = scaler)\n",
    "mape_valid = compute_mape(y_true = y_true_valid, y_hat_trend = y_hat_trend_valid, y_hat_seas = y_hat_seas_valid, scaler = scaler)\n",
    "mape_test = compute_mape(y_true = y_true_test, y_hat_trend = y_hat_trend_test, y_hat_seas = y_hat_seas_test, scaler = scaler)\n",
    "#\n",
    "plt.figure(figsize = [10, 6])\n",
    "plt.plot(np.unique(date_y_test), y_true_test, label = 'True', color = 'r')\n",
    "plt.plot(np.unique(date_y_test), y_hat_trend_test + y_hat_seas_test - scaler.mean_, label = 'Predicted', color = 'b')\n",
    "plt.plot(np.unique(date_y_test), y_hat_trend_test, label = 'Trend', color = 'b', ls = '--')\n",
    "plt.plot(np.unique(date_y_test), y_hat_seas_test - 2*scaler.mean_/2, label = 'Seasonality', color = 'b', ls = ':')\n",
    "plt.xlabel('Date', fontsize = 16)\n",
    "plt.ylabel('Sales', fontsize = 16)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.legend()\n",
    "plt.savefig('../docs/figures_for_readme/result', bbox_inches = 'tight')\n",
    "plt.show()\n",
    "#\n",
    "plt.figure(figsize = [10, 6])\n",
    "plt.plot(list_loss_train, c = 'r', label = 'Training')\n",
    "plt.plot(list_loss_valid, c = 'b', label = 'Validation')\n",
    "plt.xlabel('Epoch', fontsize = 16)\n",
    "plt.ylabel('Loss', fontsize = 16)\n",
    "plt.legend()\n",
    "plt.savefig('../docs/figures_for_readme/loss', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
